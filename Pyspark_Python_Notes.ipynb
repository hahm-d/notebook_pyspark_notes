{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bird', 'camel', 'Cat', 'Zebra']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple = ['Cat','camel','bird','Zebra']\n",
    "sorted(simple, key=lambda arg: arg.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cat']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter()\n",
    "list(filter(lambda arg: len(arg) < 4, simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAT', 'CAMEL', 'BIRD', 'ZEBRA']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map()\n",
    "list(map(lambda arg: arg.upper(), simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce()\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CatcamelbirdZebra'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda val1, val2: val1 + val2, simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/daniel/spark-2.4.7-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to interact with Pyspark, create specialized data structures called \n",
    "#### Resilient Distributed Datasets (RDD)\n",
    "#### RDDs hide all the complexity of transforming and distributing data automatically across multiple nodes\n",
    "#### SparkContext object allows you to connect to a Spark cluster and create RDDs.\n",
    "#### The local[*] string is a special string denoting that you are using a local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a SparkContext can be more involved when using a cluster. To connect to a Spark cluster, apply some authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fad6c1725f8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster('spark://head_node:56887')\n",
    "conf.set('spark.authenticate', True)\n",
    "conf.set('spark.authenticate.secret', 'secret-key')\n",
    "# sc = SparkContext(conf=conf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parallelize() can transform some Python data structures like lists and tuples into RDD's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code creates an iterator of 10,000 elements and then uses parallelize() to distribute that data into 2 partitions.\n",
    "big_list = range(10000)\n",
    "rdd = sc.parallelize(big_list,2)\n",
    "odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "odds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: By using RDD filter() method (not built-in filter()) that operation occurs in a distributed manner across several CPUs or computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing Functions to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass(object):\n",
    "    def func(self, s):\n",
    "        return s\n",
    "    def doStuff(self, rdd):\n",
    "        return rdd.map(self.func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest way to copy  field into a local variable instead of accessing it externally:\n",
    "def doStuff(self,rdd):\n",
    "    field =self.field\n",
    "    return rdd.map(lambda s: field + s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding closures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the harder things about Spark is understanding the scope and life cycle of variables and methods then executing code across a cluster.\n",
    "### RDD operations that modify veriables outside of their scope can be a frequent source of confusion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter value:  0\n"
     ]
    }
   ],
   "source": [
    "# example of what NOT to DO!\n",
    "# stack overflow 'Understanding closures and parallelism in Spark'\n",
    "data = [1,2,3,4,5]\n",
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += 1\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"counter value: \", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cause of issue, lifecycle of the variable counter.\n",
    "#### assume a spark cluster with 1 driver and 2 executor nodes\n",
    "#### each executor has its own copy of the variables and methods, all updates will take place in that copy only!\n",
    "#### the counter on the driver will still be zero because the executors were modifying their own copy and the counter variable on the driver is untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead... use Accumulators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### closure are those variables and methods which much be visible for the executor to perform its computations on the RDD.\n",
    "#### next time, before updating a variable in a loop while dealing with Spark, think about its scope and how Spark will break down the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets apply the accumulator\n",
    "num = sc.accumulator(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter value:  15\n"
     ]
    }
   ],
   "source": [
    "def inc_counter(x):\n",
    "    global num\n",
    "    num += x\n",
    "\n",
    "rdd = sc.parallelize([2,3,4,5])\n",
    "rdd.foreach(inc_counter)\n",
    "print(\"counter value: \", num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Serializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For performance tuningon Apache Spark, Serialization is used. However, all data which is sent over the network or written to the disk or persisted in memory must be serialized.\n",
    "### PySpark supports two types of serializers: MarshalSerializer & PickleSerializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Level in Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark StorageLevel decides how RDD should be stored in Apache Spark (stored in memory or disk).\n",
    "### Additionally it decides whether to serialize RDD and to replicate RDD partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
